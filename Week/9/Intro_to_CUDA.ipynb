{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qh6QIvYj-r"
      },
      "source": [
        "# Introduction to CUDA\n",
        "\n",
        "Introduction to CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVIxfXFcZTcn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "- Check CUDA version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hhyddgZCDE",
        "outputId": "50050bdf-3107-4b95-d35b-7b0c7aef0d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EXSdpEZvRo"
      },
      "source": [
        "- Install C-language support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LxwAnkjZxZs",
        "outputId": "4ea6e5c4-a3b5-466f-8a8b-6cc037d92372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-dq9d1x67\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-dq9d1x67\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=775da20a813f016bebc08bac5889c93333afbf532d71b81a5b95ff97c7447b56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d41ixfmj/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffo_EnQOZ58L"
      },
      "source": [
        "- Load plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inXLYQ1XZ8zT",
        "outputId": "35456a2b-f1c2-4934-e37d-92c8d1d34f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AG5MKboaTUH"
      },
      "source": [
        "## Hello World program\n",
        "\n",
        "- Hello World program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk5ULgaqa2nP",
        "outputId": "5e0c5f48-2054-476c-9695-6c57c7d63a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from the CPU.\n",
            "Hello from the GPU.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "// First line == using CUDA libraries in CoLab\n",
        "#include <stdio.h>\n",
        "\n",
        "void helloCPU()\n",
        "{\n",
        "  printf(\"Hello from the CPU.\\n\");\n",
        "}\n",
        "\n",
        "/*\n",
        " * The addition of `__global__` signifies that this function\n",
        " * should be launced on the GPU.\n",
        " */\n",
        "\n",
        "__global__ void helloGPU()\n",
        "{\n",
        "  printf(\"Hello from the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int x= 3;\n",
        "   helloCPU();\n",
        "\n",
        "\n",
        "  /*\n",
        "   * Add an execution configuration with the <<<...>>> syntax\n",
        "   * will launch this function as a kernel on the GPU.\n",
        "   */\n",
        "\n",
        "  helloGPU<<<1, 1>>>();\n",
        "\n",
        "  /*\n",
        "   * `cudaDeviceSynchronize` will block the CPU stream until\n",
        "   * all GPU kernels have completed.\n",
        "   */\n",
        "\n",
        "\n",
        "  cudaDeviceSynchronize(); // Doesn't work without this line\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5fH_D7kcK4t"
      },
      "source": [
        "## SAXPY Program\n",
        "\n",
        "- SAXPY Program. SAXPY stands for \"Single-Precision A-X Plus Y\"\n",
        "\n",
        "  $z = ax + y$  \n",
        "  $x, y, z : vector$  \n",
        "  $a : scalar$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEb0-YBLcxKO",
        "outputId": "edfe9f97-b49d-4cab-aee8-f6886376cfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "GPU active.....\n",
            "Max error: 0.000000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "// Using CUDA libraries in CoLab\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ // global function\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  printf(\"GPU active.....\\n\");\n",
        "\n",
        "  int i = blockIdx.x*blockDim.x + threadIdx.x; // gives you an id of where you are\n",
        "  if (i < n) y[i] = a*x[i] + y[i]; // SAXPY equation\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 100; // chosen block size\n",
        "  float *x, *y, *d_x, *d_y; // *d_x, *d_y: Convention for device, x & y on device\n",
        "  x = (float*)malloc(N*sizeof(float)); // creating vector x holding N elements\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  cudaMalloc(&d_x, N*sizeof(float)); // create vector x holding N elements on the device\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) { // fill up N values on the CPU\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice); // copy from host to device (GPU) - x into d_x\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Perform SAXPY on 100 elements\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y); // even number of block numbers\n",
        "\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost); // copy from device (GPU) to host (CPU) - d_y to y\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // free memory\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc_H-UkjdNWq"
      },
      "source": [
        "### References\n",
        "\n",
        "- References\n",
        "  - [NVIDIA Developer: Six Ways to SAXPY](https://developer.nvidia.com/blog/six-ways-saxpy/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTCr-w6YfCM"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
